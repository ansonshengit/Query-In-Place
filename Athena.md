# Athena Session
https://aws.amazon.com/blogs/big-data/build-a-data-lake-foundation-with-aws-glue-and-amazon-s3/
In this session, you will do the following:
1. Define a database.
2. Configure a crawler to explore data in an Amazon S3 bucket, and create a table fromt it. 
3. Transform the CSV file into Parquet, configure another crawler and create another table for the Parquet data, 
4. Query the data with Amazon Athena on the two tables created above. 

## Discover the data as is and query in place
Sign in to the AWS Management Console and open the AWS Glue console. You can find AWS Glue in the Analytics section. Before building this solution, please choose the us-east-1 AWS Region. 

The first step to discovering the data is to add a database. A database is a collection of tables.

1. In the console, choose Add database. In Database name, type nycitytaxi, and choose Create.

2. Choose Tables in the navigation pane. A table consists of the names of columns, data type definitions, and other metadata about a dataset.

3. Add a table to the database nycitytaxi.You can add a table manually or by using a crawler. A crawler is a program that connects to a data store and progresses through a prioritized list of classifiers to determine the schema for your data. AWS Glue provides classifiers for common file types like CSV, JSON, Avro, and others. You can also write your own classifier using a grok pattern.

4. To add a crawler, enter the data source: an Amazon S3 bucket named s3://aws-bigdata-blog/artifacts/glue-data-lake/data/. This S3 bucket contains the data file consisting of all the rides for the green taxis for the month of January 2017.

5. Choose Next.

6. For IAM role, choose the default role AWSGlueServiceRoleDefault in the drop-down list.

7. For Frequency, choose Run on demand. The crawler can be run on demand or set to run on a schedule.

8. For Database, choose nycitytaxi.It is important to understand how AWS Glue deals with schema changes so that you can select the appropriate method. In this example, the table is updated with any change. For more information about schema changes, see Cataloging Tables with a Crawler in the AWS Glue Developer Guide.

9. Review the steps, and choose Finish. The crawler is ready to run. Choose Run it now.

    When the crawler has finished, one table has been added.

10. Choose Tables in the left navigation pane, and then choose data. This screen describes the table, including schema, properties, and other valuable information.

11. You can query the data using standard SQL.

    Choose the nytaxigreenparquet
    Type `sql Select * From "nycitytaxi"."data" limit 10;`
    Choose Run Query.


## Transform the data from CSV to Parquet format, and query in place
Now you can configure and run a job to transform the data from CSV to Parquet. Parquet is a columnar format that is well suited for AWS analytics services like Amazon Athena and Amazon Redshift Spectrum.

1. Under ETL in the left navigation pane, choose Jobs, and then choose Add job.
2. For the Name, type nytaxi-csv-parquet.
3. For the IAM role, choose AWSGlueServiceRoleDefault.
4. For This job runs, choose A proposed script generated by AWS Glue.
5. Provide a unique Amazon S3 path to store the scripts.
6. Provide a unique Amazon S3 directory for a temporary directory.
7. Choose Next.
8. Choose data as the data source.
9. Choose Create tables in your data target.
10. Choose Parquet as the format.
11. Choose a new location (a new prefix location without any existing objects) to store the results.
12. Verify the schema mapping, and choose Finish.
13. View the job.This screen provides a complete view of the job and allows you to edit, save, and run the job.AWS Glue created this script. However, if required, you can create your own.
14. Choose Save, and then choose Run job.

Add the Parquet table and crawler
When the job has finished, add a new table for the Parquet data using a crawler.

1. For Crawler name, type nytaxiparquet.
2. Choose S3 as the Data store.
3. Include the Amazon S3 path chosen in the ETL
4. For the IAM role, choose AWSGlueServiceRoleDefault.
5. For Database, choose nycitytaxi.
6. For Frequency, choose Run on demand.

After the crawler has finished, there are two tables in the nycitytaxi database: a table for the raw CSV data and a table for the transformed Parquet data.

You can query the data using standard SQL.

    Choose the nytaxigreenparquet
    Type `sql Select * From "nycitytaxi"."data" limit 10;`
    Choose Run Query.
    
Conclusion
This post demonstrates how easy it is to build the foundation of a data lake using AWS Glue and Amazon S3. By using AWS Glue to crawl your data on Amazon S3 and build an Apache Hive-compatible metadata store, you can use the metadata across the AWS analytic services and popular Hadoop ecosystem tools. This combination of AWS services is powerful and easy to use, allowing you to get to business insights faster.

If you have questions or suggestions, please comment below.
